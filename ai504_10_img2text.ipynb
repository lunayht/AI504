{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "AI504_10_Img2Text_Sol.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "boMjsD-_Pt7h"
      },
      "source": [
        "**Reference : [Tutorial for Img2Text](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning)**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hX9ENrd5TNxA"
      },
      "source": [
        "# 0. Preliminary\n",
        "In this practice, we'll cover\n",
        "\n",
        "1. Preprocessing image captioning data\n",
        "2. Implement show, attend and tell\n",
        "3. Train & Evaluate\n",
        "4. Generate captions & Visualize attentions with beam search"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6MbNFRkwHyo"
      },
      "source": [
        "**0-1. Download dataset**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ewff6aj8ECmO"
      },
      "source": [
        "!gdown --id 1dTaKZ1h__X26uIzyo388L7cHO4TjC0pX\n",
        "!unzip practice_week10.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PbE6IIWcTSe_"
      },
      "source": [
        "**0-2. Dataset description**\n",
        "\n",
        "We will use Flickr8k for training and subset of MS COCO for evaluation. \n",
        "\n",
        "Flickr8k is a labeled dataset consisting of 8000 photos with 5 captions for each photos. It includes images obtained from the Flickr website. \n",
        "\n",
        "MS COCO is a large-scale object detection, segmentation, and captioning dataset. It also contains photos with 5 captions for each photo, but the size of the dataset is much larger than Flickr8k (~13GB).\n",
        "\n",
        "Below is the example from MS COCO dataset. You can also explore MS COCO [here](https://cocodataset.org/#explore)\n",
        "\n",
        "__Image__:\n",
        "\n",
        "![picture](https://farm1.staticflickr.com/68/202184509_c806481c8f_z.jpg)\n",
        "\n",
        "__Captions__:\n",
        "* the orange cat looks at a television set in a chair.\n",
        "* a cat on a wood floor watching a tv sitting on a chair.\n",
        "* a cat is sitting on the floor and watching television.\n",
        "* a fat cat in the living room watching the tv\n",
        "* a cat on the floor watching a tv on a chair.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zC0uSs6IxH8"
      },
      "source": [
        "# 1. Preprocess image captioning data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s_fm1CyhBSNn"
      },
      "source": [
        "**1-1. Preprocess dataset and cache them into local disk**\n",
        "\n",
        "Creates input files for training, validation, and test data.\n",
        "\n",
        "**For image**, we're using a pretrained Encoder, we would need to process the images into the form this pretrained Encoder is accustomed to.\n",
        "\n",
        "The pixel values must be in the range [0,1] and we must then normalize the image by the mean and standard deviation of the ImageNet images' RGB channels.\n",
        "\n",
        "```\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std = [0.229, 0.224, 0.225]\n",
        "```\n",
        "\n",
        "\n",
        "We will resize all images to 256x256 for uniformity.\n",
        "\n",
        "**For text**, we'll tokenize sentences and add special tokens.\n",
        "\n",
        "```\n",
        "before: a man holds a football\n",
        "after : <start> a man holds a football <end> <pad> <pad> <pad>....\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj_iX7kh4Ncp"
      },
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import h5py\n",
        "import json\n",
        "import torch\n",
        "from skimage.io import imread\n",
        "from skimage.transform import resize\n",
        "from tqdm import tqdm\n",
        "from collections import Counter\n",
        "from random import seed, choice, sample\n",
        "\n",
        "def create_input_files(dataset='flickr8k', \n",
        "                       db_configure_path='caption_datasets/dataset_flickr8k.json', \n",
        "                       image_folder='Flicker8k_Dataset', \n",
        "                       captions_per_image=5, \n",
        "                       min_word_freq=5, \n",
        "                       output_folder='cache_features',\n",
        "                       max_len=50):\n",
        "\n",
        "    assert dataset in {'coco', 'flickr8k', 'flickr30k'}\n",
        "\n",
        "    # Read DB configuration file\n",
        "    with open(db_configure_path, 'r') as j:\n",
        "        data = json.load(j)\n",
        "\n",
        "    # Read image paths and captions for each image\n",
        "    train_image_paths = []\n",
        "    train_image_captions = []\n",
        "    val_image_paths = []\n",
        "    val_image_captions = []\n",
        "    test_image_paths = []\n",
        "    test_image_captions = []\n",
        "    word_freq = Counter()\n",
        "\n",
        "    for img in data['images']:\n",
        "        captions = []\n",
        "        for c in img['sentences']:\n",
        "            # Update word frequency\n",
        "            word_freq.update(c['tokens'])\n",
        "            if len(c['tokens']) <= max_len:\n",
        "                captions.append(c['tokens'])\n",
        "\n",
        "        if len(captions) == 0:\n",
        "            continue\n",
        "\n",
        "        path = os.path.join(image_folder, img['filepath'], img['filename']) if dataset == 'coco' else os.path.join(\n",
        "            image_folder, img['filename'])\n",
        "\n",
        "        if img['split'] in {'train', 'restval'}:\n",
        "            train_image_paths.append(path)\n",
        "            train_image_captions.append(captions)\n",
        "        elif img['split'] in {'val'}:\n",
        "            val_image_paths.append(path)\n",
        "            val_image_captions.append(captions)\n",
        "        elif img['split'] in {'test'}:\n",
        "            test_image_paths.append(path)\n",
        "            test_image_captions.append(captions)\n",
        "\n",
        "    # Sanity check\n",
        "    assert len(train_image_paths) == len(train_image_captions)\n",
        "    assert len(val_image_paths) == len(val_image_captions)\n",
        "    assert len(test_image_paths) == len(test_image_captions)\n",
        "\n",
        "    # Create vocab\n",
        "    words = [w for w in word_freq.keys() if word_freq[w] > min_word_freq]\n",
        "    vocab = {k: v + 1 for v, k in enumerate(words)}\n",
        "    vocab['<unk>'] = len(vocab) + 1\n",
        "    vocab['<start>'] = len(vocab) + 1\n",
        "    vocab['<end>'] = len(vocab) + 1\n",
        "    vocab['<pad>'] = 0\n",
        "\n",
        "    # Create a base/root name for all output files\n",
        "    base_filename = dataset + '_' + str(captions_per_image) + '_cap_per_img_' + str(min_word_freq) + '_min_word_freq'\n",
        "\n",
        "    if not os.path.isdir(output_folder):\n",
        "      os.mkdir(output_folder)\n",
        "    # Save word map to a JSON\n",
        "    with open(os.path.join(output_folder, 'WORDMAP_' + base_filename + '.json'), 'w') as j:\n",
        "        json.dump(vocab, j)\n",
        "\n",
        "    # Sample captions for each image, save images to HDF5 file, and captions and their lengths to JSON files\n",
        "    seed(123)\n",
        "    for impaths, imcaps, split in [(train_image_paths, train_image_captions, 'TRAIN'),\n",
        "                                   (val_image_paths, val_image_captions, 'VAL'),\n",
        "                                   (test_image_paths, test_image_captions, 'TEST')]:\n",
        "\n",
        "        with h5py.File(os.path.join(output_folder, split + '_IMAGES_' + base_filename + '.hdf5'), 'a') as h:\n",
        "            # Make a note of the number of captions we are sampling per image\n",
        "            h.attrs['captions_per_image'] = captions_per_image\n",
        "\n",
        "            # Create dataset inside HDF5 file to store images\n",
        "            images = h.create_dataset('images', (len(impaths), 3, 256, 256), dtype='uint8')\n",
        "\n",
        "            print(\"\\nReading %s images and captions, storing to file...\\n\" % split)\n",
        "\n",
        "            enc_captions = []\n",
        "            caplens = []\n",
        "\n",
        "            for i, path in enumerate(tqdm(impaths)):\n",
        "\n",
        "                # Sample captions\n",
        "                if len(imcaps[i]) < captions_per_image:\n",
        "                    captions = imcaps[i] + [choice(imcaps[i]) for _ in range(captions_per_image - len(imcaps[i]))]\n",
        "                else:\n",
        "                    captions = sample(imcaps[i], k=captions_per_image)\n",
        "\n",
        "                # Sanity check\n",
        "                assert len(captions) == captions_per_image\n",
        "                #print(impaths[i])\n",
        "                # Read images\n",
        "                \n",
        "                img = imread(impaths[i])\n",
        "                if len(img.shape) == 2:\n",
        "                    img = img[:, :, np.newaxis]\n",
        "                    img = np.concatenate([img, img, img], axis=2)\n",
        "                img = resize(img, (256, 256))\n",
        "                img = img.transpose(2, 0, 1)\n",
        "                assert img.shape == (3, 256, 256)\n",
        "                assert np.max(img) <= 255\n",
        "\n",
        "                # Save image to HDF5 file\n",
        "                images[i] = img\n",
        "\n",
        "                for j, c in enumerate(captions):\n",
        "                    # Encode captions\n",
        "                    enc_c = [vocab['<start>']] + [vocab.get(word, vocab['<unk>']) for word in c] + [\n",
        "                        vocab['<end>']] + [vocab['<pad>']] * (max_len - len(c))\n",
        "\n",
        "                    # Find caption lengths\n",
        "                    c_len = len(c) + 2\n",
        "\n",
        "                    enc_captions.append(enc_c)\n",
        "                    caplens.append(c_len)\n",
        "\n",
        "            # Sanity check\n",
        "            assert images.shape[0] * captions_per_image == len(enc_captions) == len(caplens)\n",
        "\n",
        "            # Save encoded captions and their lengths to JSON files\n",
        "            with open(os.path.join(output_folder, split + '_CAPTIONS_' + base_filename + '.json'), 'w') as j:\n",
        "                json.dump(enc_captions, j)\n",
        "\n",
        "            with open(os.path.join(output_folder, split + '_CAPLENS_' + base_filename + '.json'), 'w') as j:\n",
        "                json.dump(caplens, j)\n",
        "\n",
        "create_input_files()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHpqS6B1H9wi"
      },
      "source": [
        "**2. Define pytorch dataset class**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R-GPP5k_Bd-x"
      },
      "source": [
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "import h5py\n",
        "import json\n",
        "import os\n",
        "\n",
        "\n",
        "class CaptionDataset(Dataset):\n",
        "    \"\"\"\n",
        "    A PyTorch Dataset class to be used in a PyTorch DataLoader to create batches.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data_folder, data_name, split, transform=None):\n",
        "        \"\"\"\n",
        "        :param data_folder: folder where data files are stored\n",
        "        :param data_name: base name of processed datasets\n",
        "        :param split: split, one of 'TRAIN', 'VAL', or 'TEST'\n",
        "        :param transform: image transform pipeline\n",
        "        \"\"\"\n",
        "        self.split = split\n",
        "        assert self.split in {'TRAIN', 'VAL', 'TEST'}\n",
        "\n",
        "        # Open hdf5 file where images are stored\n",
        "        self.h = h5py.File(os.path.join(data_folder, self.split + '_IMAGES_' + data_name + '.hdf5'), 'r')\n",
        "        self.imgs = self.h['images']\n",
        "\n",
        "        # Captions per image\n",
        "        self.cpi = self.h.attrs['captions_per_image']\n",
        "\n",
        "        # Load encoded captions (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPTIONS_' + data_name + '.json'), 'r') as j:\n",
        "            self.captions = json.load(j)\n",
        "\n",
        "        # Load caption lengths (completely into memory)\n",
        "        with open(os.path.join(data_folder, self.split + '_CAPLENS_' + data_name + '.json'), 'r') as j:\n",
        "            self.caplens = json.load(j)\n",
        "\n",
        "        # PyTorch transformation pipeline for the image (normalizing, etc.)\n",
        "        self.transform = transform\n",
        "\n",
        "        # Total number of datapoints\n",
        "        self.dataset_size = len(self.captions)\n",
        "\n",
        "    def __getitem__(self, i):\n",
        "        # Remember, the Nth caption corresponds to the (N // captions_per_image)th image\n",
        "        img = torch.FloatTensor(self.imgs[i // self.cpi] / 255.)\n",
        "        if self.transform is not None:\n",
        "            img = self.transform(img)\n",
        "\n",
        "        caption = torch.LongTensor(self.captions[i])\n",
        "\n",
        "        caplen = torch.LongTensor([self.caplens[i]])\n",
        "\n",
        "        if self.split is 'TRAIN':\n",
        "            return img, caption, caplen\n",
        "        else:\n",
        "            # For validation of testing, also return all 'captions_per_image' captions to find BLEU-4 score\n",
        "            all_captions = torch.LongTensor(\n",
        "                self.captions[((i // self.cpi) * self.cpi):(((i // self.cpi) * self.cpi) + self.cpi)])\n",
        "            return img, caption, caplen, all_captions\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.dataset_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZjdSIQm50r8D"
      },
      "source": [
        "# 2. Implement Show, Attend and Tell"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TAElnIGGnMkG"
      },
      "source": [
        "**2-1. Image encoder**\n",
        "\n",
        "The Encoder encodes the input image with 3 color channels into a smaller image with \"learned\" channels.\n",
        "![picture](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/raw/master/img/encoder.png)\n",
        "This smaller encoded image is a summary representation of all that's useful in the original image.\n",
        "\n",
        "Due to the VRAM limitation of colab, we'll use ResNet-101 instead of VGG16.\n",
        "![picture](https://miro.medium.com/max/1000/1*6hF97Upuqg_LdsqWY6n_wg.png)\n",
        "Since the last layer or two of this model are linear layers coupled with softmax activation for classification, we strip them away."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8yyq_tRmqt9_"
      },
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import torchvision\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "class Encoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Encoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoded_image_size=14):\n",
        "        super(Encoder, self).__init__()\n",
        "        self.enc_image_size = encoded_image_size\n",
        "\n",
        "        # Load pretrained ImageNet ResNet-101\n",
        "        resnet = torchvision.models.resnet101(pretrained=True, progress=False)\n",
        "\n",
        "        # Remove top-2 layers in model, FC linear and global average pooling layers\n",
        "        modules = list(resnet.children())[:-2]\n",
        "        self.resnet = nn.Sequential(*modules)\n",
        "\n",
        "        # Resize image to fixed size to allow input images of variable size\n",
        "        self.adaptive_pool = nn.AdaptiveAvgPool2d((encoded_image_size, encoded_image_size))\n",
        "\n",
        "\n",
        "    def forward(self, images):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param images: images, a tensor of dimensions (batch_size, 3, image_size, image_size)\n",
        "        :return: encoded images\n",
        "        \"\"\"\n",
        "        out = self.resnet(images)  # (batch_size, 2048, image_size/32, image_size/32)\n",
        "        out = self.adaptive_pool(out)  # (batch_size, 2048, encoded_image_size, encoded_image_size)\n",
        "        out = out.permute(0, 2, 3, 1)  # (batch_size, encoded_image_size, encoded_image_size, 2048)\n",
        "        return out\n",
        "\n",
        "    def fine_tune(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow or prevent the computation of gradients for convolutional blocks 2 through 4 of the encoder.\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.resnet.parameters():\n",
        "            p.requires_grad = False\n",
        "        # If fine-tuning, only fine-tune high level feature encoder\n",
        "        for c in list(self.resnet.children())[5:]:\n",
        "            for p in c.parameters():\n",
        "                p.requires_grad = fine_tune"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tc2THX1Toocl"
      },
      "source": [
        "**2-2. Attention network**\n",
        "\n",
        "The Attention network estimate the importance of a certain part of an image. \n",
        "It considers the sequence generated thus far, and attends to the part of the image that needs describing next.\n",
        "\n",
        "![picture](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/raw/master/img/att.png)\n",
        "\n",
        "We will use a soft attention, where the weights of the pixels add up to 1. If there are P pixels in our encoded image, then at each timestep t –\n",
        "\n",
        "This entire process as computing the probability that a pixel is the place to look to generate the next word.\n",
        "\n",
        "*We'll not going to implement a hard attention in this practice.*\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6K-CSacbnxdk"
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    \"\"\"\n",
        "    Attention Network.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, encoder_dim, decoder_dim, attention_dim):\n",
        "        \"\"\"\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param attention_dim: size of the attention network\n",
        "        \"\"\"\n",
        "        super(Attention, self).__init__()\n",
        "        self.encoder_map = nn.Linear(encoder_dim, attention_dim)  # linear layer to transform encoded image\n",
        "        self.decoder_map = nn.Linear(decoder_dim, attention_dim)  # linear layer to transform decoder's output\n",
        "        self.full_att = nn.Linear(attention_dim, 1)  # linear layer to calculate values to be softmax-ed\n",
        "        self.relu = nn.ReLU()\n",
        "        self.softmax = nn.Softmax(dim=1)  # softmax layer to calculate weights\n",
        "\n",
        "    def forward(self, encoder_out, decoder_hidden):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :param decoder_hidden: previous decoder output, a tensor of dimension (batch_size, decoder_dim)\n",
        "        :return: attention weighted encoding, weights\n",
        "        \"\"\"\n",
        "        enc_feat = self.encoder_map(encoder_out)  # (batch_size, num_pixels, attention_dim)\n",
        "        dec_feat = self.decoder_map(decoder_hidden)  # (batch_size, attention_dim)\n",
        "        att = self.full_att(self.relu(enc_feat + dec_feat.unsqueeze(1))).squeeze(2)  # (batch_size, num_pixels)\n",
        "        alpha = self.softmax(att)  # (batch_size, num_pixels)\n",
        "        attention_weighted_encoding = (encoder_out * alpha.unsqueeze(2)).sum(dim=1)  # (batch_size, encoder_dim)\n",
        "\n",
        "        return attention_weighted_encoding, alpha\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EcH-CMYFoo_j"
      },
      "source": [
        "**2-3. Text decoder with attention network**\n",
        "\n",
        "The Decoder's job is to look at the encoded image and generate a caption word by word.\n",
        "\n",
        "Since it's generating a sequence, so we will use Recurrent Neural Network, especially LSTM.\n",
        "\n",
        "In a setting with Attention, we want the Decoder to be able to look at different parts of the image at different points in the sequence. \n",
        "\n",
        "![picture](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/raw/master/img/decoder_att.png)\n",
        "\n",
        "Instead of the simple average, we use the weighted average across all pixels, with the weights of the important pixels being greater. This weighted representation of the image can be concatenated with the previously generated word at each step to generate the next word.\n",
        "\n",
        "One technique to speeding-up RNN is ignoring padding tokens during the recurrence. PyTorch also supports this, [pack_padded_sequence](https://pytorch.org/docs/stable/generated/torch.nn.utils.rnn.pack_padded_sequence.html)\n",
        "![picture](https://miro.medium.com/max/668/1*WF93EuCOGU834ENSnnofZg.png)\n",
        "\n",
        "We will manually implement pack_padded_sequence and text decoder with attention in this section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PwqZdnuOnxUl"
      },
      "source": [
        "class DecoderWithAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Decoder.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, attention_dim, embed_dim, decoder_dim, vocab_size, encoder_dim=2048, dropout=0.5):\n",
        "        \"\"\"\n",
        "        :param attention_dim: size of attention network\n",
        "        :param embed_dim: embedding size\n",
        "        :param decoder_dim: size of decoder's RNN\n",
        "        :param vocab_size: size of vocabulary\n",
        "        :param encoder_dim: feature size of encoded images\n",
        "        :param dropout: dropout\n",
        "        \"\"\"\n",
        "        super(DecoderWithAttention, self).__init__()\n",
        "\n",
        "        self.encoder_dim = encoder_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.embed_dim = embed_dim\n",
        "        self.decoder_dim = decoder_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.dropout = dropout\n",
        "\n",
        "        self.attention = Attention(encoder_dim, decoder_dim, attention_dim)  # attention network\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embed_dim)  # embedding layer\n",
        "        self.dropout = nn.Dropout(p=self.dropout)\n",
        "        self.decode_step = nn.LSTMCell(embed_dim + encoder_dim, decoder_dim, bias=True)  # decoding LSTMCell\n",
        "        self.init_h = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial hidden state of LSTMCell\n",
        "        self.init_c = nn.Linear(encoder_dim, decoder_dim)  # linear layer to find initial cell state of LSTMCell\n",
        "        self.f_beta = nn.Linear(decoder_dim, encoder_dim)  # linear layer to create a sigmoid-activated gate\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "        self.fc = nn.Linear(decoder_dim, vocab_size)  # linear layer to find scores over vocabulary\n",
        "        self.init_weights()  # initialize some layers with the uniform distribution\n",
        "\n",
        "    def init_weights(self):\n",
        "        \"\"\"\n",
        "        Initializes some parameters with values from the uniform distribution, for easier convergence.\n",
        "        \"\"\"\n",
        "        self.embedding.weight.data.uniform_(-0.1, 0.1)\n",
        "        self.fc.bias.data.fill_(0)\n",
        "        self.fc.weight.data.uniform_(-0.1, 0.1)\n",
        "\n",
        "    def load_pretrained_embeddings(self, embeddings):\n",
        "        \"\"\"\n",
        "        Loads embedding layer with pre-trained embeddings.\n",
        "        :param embeddings: pre-trained embeddings\n",
        "        \"\"\"\n",
        "        self.embedding.weight = nn.Parameter(embeddings)\n",
        "\n",
        "    def fine_tune_embeddings(self, fine_tune=True):\n",
        "        \"\"\"\n",
        "        Allow fine-tuning of embedding layer? (Only makes sense to not-allow if using pre-trained embeddings).\n",
        "        :param fine_tune: Allow?\n",
        "        \"\"\"\n",
        "        for p in self.embedding.parameters():\n",
        "            p.requires_grad = fine_tune\n",
        "\n",
        "    def init_hidden_state(self, encoder_out):\n",
        "        \"\"\"\n",
        "        Creates the initial hidden and cell states for the decoder's LSTM based on the encoded images.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, num_pixels, encoder_dim)\n",
        "        :return: hidden state, cell state\n",
        "        \"\"\"\n",
        "        mean_encoder_out = encoder_out.mean(dim=1)\n",
        "        h = self.init_h(mean_encoder_out)  # (batch_size, decoder_dim)\n",
        "        c = self.init_c(mean_encoder_out)\n",
        "        return h, c\n",
        "\n",
        "    def forward(self, encoder_out, encoded_captions, caption_lengths):\n",
        "        \"\"\"\n",
        "        Forward propagation.\n",
        "        :param encoder_out: encoded images, a tensor of dimension (batch_size, enc_image_size, enc_image_size, encoder_dim)\n",
        "        :param encoded_captions: encoded captions, a tensor of dimension (batch_size, max_caption_length)\n",
        "        :param caption_lengths: caption lengths, a tensor of dimension (batch_size, 1)\n",
        "        :return: scores for vocabulary, sorted encoded captions, decode lengths, weights, sort indices\n",
        "        \"\"\"\n",
        "\n",
        "        batch_size = encoder_out.size(0)\n",
        "        encoder_dim = encoder_out.size(-1)\n",
        "        vocab_size = self.vocab_size\n",
        "\n",
        "        # Flatten image\n",
        "        encoder_out = encoder_out.view(batch_size, -1, encoder_dim)  # (batch_size, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # Sort input data by decreasing lengths; why? apparent below\n",
        "        caption_lengths, sort_ind = caption_lengths.squeeze(1).sort(dim=0, descending=True)\n",
        "        encoder_out = encoder_out[sort_ind]\n",
        "        encoded_captions = encoded_captions[sort_ind]\n",
        "\n",
        "        # Since we decoded starting with <start>, the targets are all words after <start>, up to <end>\n",
        "        targets = encoded_captions[:,1:]\n",
        "\n",
        "        # Embedding\n",
        "        embeddings = self.embedding(encoded_captions)  # (batch_size, max_caption_length, embed_dim)\n",
        "\n",
        "        # Initialize LSTM state\n",
        "        h, c = self.init_hidden_state(encoder_out)  # (batch_size, decoder_dim)\n",
        "\n",
        "        # We won't decode at the <end> position, since we've finished generating as soon as we generate <end>\n",
        "        # So, decoding lengths are actual lengths - 1\n",
        "        decode_lengths = (caption_lengths - 1).tolist()\n",
        "\n",
        "        # Create tensors to hold word predicion scores and alphas\n",
        "        predictions = torch.zeros(batch_size, targets.size(1), vocab_size).to(device)\n",
        "        alphas = torch.zeros(batch_size, targets.size(1), num_pixels).to(device)\n",
        "\n",
        "        # At each time-step, decode by\n",
        "        # attention-weighing the encoder's output based on the decoder's previous hidden state output\n",
        "        # then generate a new word in the decoder with the previous word and the attention weighted encoding\n",
        "        for t in range(max(decode_lengths)):\n",
        "            batch_size_t = sum([l > t for l in decode_lengths])\n",
        "            attention_weighted_encoding, alpha = self.attention(encoder_out[:batch_size_t],\n",
        "                                                                h[:batch_size_t])\n",
        "            gate = self.sigmoid(self.f_beta(h[:batch_size_t]))  # gating scalar, (batch_size_t, encoder_dim)\n",
        "            attention_weighted_encoding = gate * attention_weighted_encoding\n",
        "            h, c = self.decode_step(\n",
        "                torch.cat([embeddings[:batch_size_t, t, :], attention_weighted_encoding], dim=1),\n",
        "                (h[:batch_size_t], c[:batch_size_t]))  # (batch_size_t, decoder_dim)\n",
        "            preds = self.fc(self.dropout(h))  # (batch_size_t, vocab_size)\n",
        "            predictions[:batch_size_t, t, :] = preds\n",
        "            alphas[:batch_size_t, t, :] = alpha\n",
        "\n",
        "        return predictions, targets, decode_lengths, alphas, sort_ind"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AqV61g7906u7"
      },
      "source": [
        "# 3. Train and Evaluate your model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "poijztIwgodn"
      },
      "source": [
        "In this section, we'll train the model and evaluate it's performance with BLEU-4 score."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8gRDfOYCQ9lj"
      },
      "source": [
        "**3.1 Set hyperparameters**\n",
        "\n",
        "Load python libraries and set some hyperparameters.\n",
        "\n",
        "Here, we will use [nltk](https://www.nltk.org/) library to calculate BLEU score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1uO9Hp8qt7W"
      },
      "source": [
        "import time\n",
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "from torch import nn\n",
        "from torch.nn.utils.rnn import pack_padded_sequence\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "from tqdm import tqdm\n",
        "from utils import *\n",
        "\n",
        "# Data parameters\n",
        "data_folder = 'cache_features'  # folder with data files saved by create_input_files.py\n",
        "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
        "\n",
        "# Model parameters\n",
        "emb_dim = 512  # dimension of word embeddings\n",
        "attention_dim = 512  # dimension of attention linear layers\n",
        "decoder_dim = 512  # dimension of decoder RNN\n",
        "dropout = 0.5\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# Training parameters\n",
        "start_epoch = 0\n",
        "epochs = 1  # number of epochs to train for (if early stopping is not triggered)\n",
        "epochs_since_improvement = 0  # keeps track of number of epochs since there's been an improvement in validation BLEU\n",
        "batch_size = 32\n",
        "workers = 1  # for data-loading; right now, only 1 works with h5py\n",
        "encoder_lr = 1e-4  # learning rate for encoder if fine-tuning\n",
        "decoder_lr = 4e-4  # learning rate for decoder\n",
        "grad_clip = 5.  # clip gradients at an absolute value of\n",
        "alpha_c = 1.  # regularization parameter for 'doubly stochastic attention', as in the paper\n",
        "best_bleu4 = 0.  # BLEU-4 score right now\n",
        "print_freq = 100  # print training/validation stats every __ batches\n",
        "fine_tune_encoder = True  # fine-tune encoder?\n",
        "checkpoint = None  # path to checkpoint, None if none"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x8MpzS69R0wr"
      },
      "source": [
        "**3.2 Train**\n",
        "\n",
        "We will implement the objective function and single training loop in [paper](https://arxiv.org/pdf/1502.03044.pdf).\n",
        "\n",
        "The objective function is:\n",
        "\n",
        "![picture](https://img1.daumcdn.net/thumb/R1280x0/?scode=mtistory2&fname=https%3A%2F%2Fblog.kakaocdn.net%2Fdn%2FcSm8FD%2FbtqvuW2U4hZ%2FKZCxHKQv3J0jLi1RBs6FB0%2Fimg.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BLP7LDmRHs7"
      },
      "source": [
        "def train(train_loader, encoder, decoder, criterion, encoder_optimizer, decoder_optimizer, epoch):\n",
        "    \"\"\"\n",
        "    Performs one epoch's training.\n",
        "    :param train_loader: DataLoader for training data\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :param encoder_optimizer: optimizer to update encoder's weights (if fine-tuning)\n",
        "    :param decoder_optimizer: optimizer to update decoder's weights\n",
        "    :param epoch: epoch number\n",
        "    \"\"\"\n",
        "\n",
        "    decoder.train()  # train mode (dropout and batchnorm is used)\n",
        "    encoder.train()\n",
        "\n",
        "    batch_time = AverageMeter()  # forward prop. + back prop. time\n",
        "    data_time = AverageMeter()  # data loading time\n",
        "    losses = AverageMeter()  # loss (per word decoded)\n",
        "    top5accs = AverageMeter()  # top5 accuracy\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    # Batches\n",
        "    for i, (imgs, caps, caplens) in tqdm(enumerate(train_loader),total=len(train_loader)):\n",
        "        data_time.update(time.time() - start)\n",
        "\n",
        "        # Move to GPU, if available\n",
        "        imgs = imgs.to(device)\n",
        "        caps = caps.to(device)\n",
        "        caplens = caplens.to(device)\n",
        "\n",
        "        # Forward prop.\n",
        "        imgs = encoder(imgs)\n",
        "        scores, targets, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "        # Resize score and targets\n",
        "        scores = scores.reshape(-1,scores.size(2))\n",
        "        targets = targets.reshape(-1)\n",
        "\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(scores, targets)\n",
        "\n",
        "        # Add doubly stochastic attention regularization\n",
        "        loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "        # Back prop.\n",
        "        decoder_optimizer.zero_grad()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip gradients\n",
        "        if grad_clip is not None:\n",
        "            clip_gradient(decoder_optimizer, grad_clip)\n",
        "            if encoder_optimizer is not None:\n",
        "                clip_gradient(encoder_optimizer, grad_clip)\n",
        "\n",
        "        # Update weights\n",
        "        decoder_optimizer.step()\n",
        "        if encoder_optimizer is not None:\n",
        "            encoder_optimizer.step()\n",
        "\n",
        "        # Keep track of metrics\n",
        "        top5 = accuracy(scores, targets, 5)\n",
        "        losses.update(loss.item(), sum(decode_lengths))\n",
        "        top5accs.update(top5, sum(decode_lengths))\n",
        "        batch_time.update(time.time() - start)\n",
        "\n",
        "        start = time.time()\n",
        "\n",
        "        # Print status\n",
        "        if i % print_freq == 0:\n",
        "            print('\\nEpoch: [{0}][{1}/{2}]\\t'\n",
        "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                  'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})'.format(epoch, i, len(train_loader),\n",
        "                                                                          loss=losses,\n",
        "                                                                          top5=top5accs))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xy0Dd1q-R0JY"
      },
      "source": [
        "**3.3 Evaluation**\n",
        "\n",
        "We will evalute the trained model with BLEU-4 score."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZdNJF3hjRIDZ"
      },
      "source": [
        "def validate(val_loader, encoder, decoder, criterion):\n",
        "    \"\"\"\n",
        "    Performs one epoch's validation.\n",
        "    :param val_loader: DataLoader for validation data.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param criterion: loss layer\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    decoder.eval()  # eval mode (no dropout or batchnorm)\n",
        "    if encoder is not None:\n",
        "        encoder.eval()\n",
        "\n",
        "    batch_time = AverageMeter()\n",
        "    losses = AverageMeter()\n",
        "    top5accs = AverageMeter()\n",
        "\n",
        "    start = time.time()\n",
        "\n",
        "    references = list()  # references (true captions) for calculating BLEU-4 score\n",
        "    hypotheses = list()  # hypotheses (predictions)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        # Batches\n",
        "        for i, (imgs, caps, caplens, allcaps) in enumerate(val_loader):\n",
        "\n",
        "            # Move to device, if available\n",
        "            imgs = imgs.to(device)\n",
        "            caps = caps.to(device)\n",
        "            caplens = caplens.to(device)\n",
        "\n",
        "            # Forward prop.\n",
        "            if encoder is not None:\n",
        "                imgs = encoder(imgs)\n",
        "            scores, targets, decode_lengths, alphas, sort_ind = decoder(imgs, caps, caplens)\n",
        "\n",
        "            # Resize score and targets\n",
        "            scores_copy = scores.clone()\n",
        "            scores = scores.reshape(-1,scores.size(2))\n",
        "            targets = targets.reshape(-1)\n",
        "\n",
        "\n",
        "            # Calculate loss\n",
        "            loss = criterion(scores, targets)\n",
        "\n",
        "            # Add doubly stochastic attention regularization\n",
        "            loss += alpha_c * ((1. - alphas.sum(dim=1)) ** 2).mean()\n",
        "\n",
        "            # Keep track of metrics\n",
        "            losses.update(loss.item(), sum(decode_lengths))\n",
        "            top5 = accuracy(scores, targets, 5)\n",
        "            top5accs.update(top5, sum(decode_lengths))\n",
        "            batch_time.update(time.time() - start)\n",
        "\n",
        "            start = time.time()\n",
        "\n",
        "            if i % print_freq == 0:\n",
        "                print('Validation: [{0}/{1}]\\t'\n",
        "                      'Batch Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
        "                      'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
        "                      'Top-5 Accuracy {top5.val:.3f} ({top5.avg:.3f})\\t'.format(i, len(val_loader), batch_time=batch_time,\n",
        "                                                                                loss=losses, top5=top5accs))\n",
        "\n",
        "            # Store references (true captions), and hypothesis (prediction) for each image\n",
        "            # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "            # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "\n",
        "            # References\n",
        "            allcaps = allcaps[sort_ind]  # because images were sorted in the decoder\n",
        "            for j in range(allcaps.shape[0]):\n",
        "                img_caps = allcaps[j].tolist()\n",
        "                img_captions = list(\n",
        "                    map(lambda c: [w for w in c if w not in {vocab['<start>'], vocab['<pad>']}],\n",
        "                        img_caps))  # remove <start> and pads\n",
        "                references.append(img_captions)\n",
        "\n",
        "            # Hypotheses\n",
        "            _, preds = torch.max(scores_copy, dim=2)\n",
        "            preds = preds.tolist()\n",
        "            temp_preds = list()\n",
        "            for j, p in enumerate(preds):\n",
        "                temp_preds.append(preds[j][:decode_lengths[j]])  # remove pads\n",
        "            preds = temp_preds\n",
        "            hypotheses.extend(preds)\n",
        "\n",
        "            assert len(references) == len(hypotheses)\n",
        "\n",
        "        # Calculate BLEU-4 scores\n",
        "        bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "        print(\n",
        "            '\\n * LOSS - {loss.avg:.3f}, TOP-5 ACCURACY - {top5.avg:.3f}, BLEU-4 - {bleu}\\n'.format(\n",
        "                loss=losses,\n",
        "                top5=top5accs,\n",
        "                bleu=bleu4))\n",
        "\n",
        "    return bleu4"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00yDLCgCSoT9"
      },
      "source": [
        "**3.4 Run training & evaluation code**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3YwkKjmIRHmW"
      },
      "source": [
        "def main():\n",
        "    \"\"\"\n",
        "    Training and validation.\n",
        "    \"\"\"\n",
        "\n",
        "    global best_bleu4, epochs_since_improvement, checkpoint, start_epoch, fine_tune_encoder, data_name, vocab\n",
        "\n",
        "    # Read word map\n",
        "    vocab_file = os.path.join(data_folder, 'WORDMAP_' + data_name + '.json')\n",
        "    with open(vocab_file, 'r') as j:\n",
        "        vocab = json.load(j)\n",
        "\n",
        "    # Initialize / load checkpoint\n",
        "    if checkpoint is None:\n",
        "        decoder = DecoderWithAttention(attention_dim=attention_dim,\n",
        "                                       embed_dim=emb_dim,\n",
        "                                       decoder_dim=decoder_dim,\n",
        "                                       vocab_size=len(vocab),\n",
        "                                       dropout=dropout)\n",
        "        decoder_optimizer = torch.optim.RMSprop(params=filter(lambda p: p.requires_grad, decoder.parameters()),\n",
        "                                             lr=decoder_lr)\n",
        "        encoder = Encoder()\n",
        "        encoder.fine_tune(fine_tune_encoder)\n",
        "        encoder_optimizer = torch.optim.RMSprop(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                             lr=encoder_lr) if fine_tune_encoder else None\n",
        "\n",
        "    else:\n",
        "        checkpoint = torch.load(checkpoint)\n",
        "        start_epoch = checkpoint['epoch'] + 1\n",
        "        epochs_since_improvement = checkpoint['epochs_since_improvement']\n",
        "        best_bleu4 = checkpoint['bleu-4']\n",
        "        decoder = checkpoint['decoder']\n",
        "        decoder_optimizer = checkpoint['decoder_optimizer']\n",
        "        encoder = checkpoint['encoder']\n",
        "        encoder_optimizer = checkpoint['encoder_optimizer']\n",
        "        if fine_tune_encoder is True and encoder_optimizer is None:\n",
        "            encoder.fine_tune(fine_tune_encoder)\n",
        "            encoder_optimizer = torch.optim.Adam(params=filter(lambda p: p.requires_grad, encoder.parameters()),\n",
        "                                                 lr=encoder_lr)\n",
        "\n",
        "    # Move to GPU, if available\n",
        "    decoder = decoder.to(device)\n",
        "    encoder = encoder.to(device)\n",
        "\n",
        "    # Loss function\n",
        "    criterion = nn.CrossEntropyLoss(ignore_index=vocab['<pad>']).to(device)\n",
        "\n",
        "    # Custom dataloaders\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TRAIN', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "    val_loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'VAL', transform=transforms.Compose([normalize])),\n",
        "        batch_size=batch_size, shuffle=True, num_workers=workers, pin_memory=True)\n",
        "\n",
        "    # Epochs\n",
        "    for epoch in range(start_epoch, epochs):\n",
        "\n",
        "        # Decay learning rate if there is no improvement for 8 consecutive epochs, and terminate training after 20\n",
        "        if epochs_since_improvement == 20:\n",
        "            break\n",
        "        if epochs_since_improvement > 0 and epochs_since_improvement % 8 == 0:\n",
        "            adjust_learning_rate(decoder_optimizer, 0.8)\n",
        "            if fine_tune_encoder:\n",
        "                adjust_learning_rate(encoder_optimizer, 0.8)\n",
        "\n",
        "        # One epoch's training\n",
        "        train(train_loader=train_loader,\n",
        "              encoder=encoder,\n",
        "              decoder=decoder,\n",
        "              criterion=criterion,\n",
        "              encoder_optimizer=encoder_optimizer,\n",
        "              decoder_optimizer=decoder_optimizer,\n",
        "              epoch=epoch)\n",
        "\n",
        "        # One epoch's validation\n",
        "        recent_bleu4 = validate(val_loader=val_loader,\n",
        "                                encoder=encoder,\n",
        "                                decoder=decoder,\n",
        "                                criterion=criterion)\n",
        "\n",
        "        # Check if there was an improvement\n",
        "        is_best = recent_bleu4 > best_bleu4\n",
        "        best_bleu4 = max(recent_bleu4, best_bleu4)\n",
        "        if not is_best:\n",
        "            epochs_since_improvement += 1\n",
        "            print(\"\\nEpochs since last improvement: %d\\n\" % (epochs_since_improvement,))\n",
        "        else:\n",
        "            epochs_since_improvement = 0\n",
        "\n",
        "        # Save checkpoint\n",
        "        save_checkpoint(data_name, epoch, epochs_since_improvement, encoder, decoder, encoder_optimizer,\n",
        "                        decoder_optimizer, recent_bleu4, is_best)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYAhQycb1Ipa"
      },
      "source": [
        "# 4. Generate captions & Visualize attentions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mofa15Y8T_uF"
      },
      "source": [
        "**4.1 Generate captions with beam search**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7YXj6ARn2lH"
      },
      "source": [
        "The greedy decoding choose the word with the highest score and use it to predict the next word. But this is not optimal because the rest of the sequence hinges on that first word you choose. If that choice isn't the best, everything that follows is sub-optimal. And it's not just the first word – each word in the sequence has consequences for the ones that succeed it.\n",
        "\n",
        "It would be best if we could somehow not decide until we've finished decoding completely, and choose the sequence that has the highest overall score from a basket of candidate sequences. Beam Search does exactly this.\n",
        "\n",
        "![picture](https://github.com/sgrvinod/a-PyTorch-Tutorial-to-Image-Captioning/raw/master/img/beam_search.png)\n",
        "\n",
        "In this section, we will implement beam search decoding scheme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LVbJ80HXT_1C"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "import json\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.cm as cm\n",
        "import skimage.transform\n",
        "from skimage import img_as_ubyte\n",
        "from skimage.io import imread\n",
        "from PIL import Image\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "\n",
        "def caption_image_beam_search(encoder, decoder, image_path, vocab, beam_size=3):\n",
        "    \"\"\"\n",
        "    Reads an image and captions it with beam search.\n",
        "    :param encoder: encoder model\n",
        "    :param decoder: decoder model\n",
        "    :param image_path: path to image\n",
        "    :param vocab: word map\n",
        "    :param beam_size: number of sequences to consider at each decode-step\n",
        "    :return: caption, weights for visualization\n",
        "    \"\"\"\n",
        "\n",
        "    k = beam_size\n",
        "    vocab_size = len(vocab)\n",
        "\n",
        "    # Read image and process\n",
        "    img = imread(image_path)\n",
        "    if len(img.shape) == 2:\n",
        "        img = img[:, :, np.newaxis]\n",
        "        img = np.concatenate([img, img, img], axis=2)\n",
        "    img = img_as_ubyte(skimage.transform.resize(img, (256, 256)))\n",
        "    img = img.transpose(2, 0, 1)\n",
        "    img = img / 255\n",
        "    img = torch.FloatTensor(img).to(device)\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                     std=[0.229, 0.224, 0.225])\n",
        "    transform = transforms.Compose([normalize])\n",
        "    image = transform(img)  # (3, 256, 256)\n",
        "\n",
        "    # Encode\n",
        "    image = image.unsqueeze(0)  # (1, 3, 256, 256)\n",
        "    encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "    enc_image_size = encoder_out.size(1)\n",
        "    encoder_dim = encoder_out.size(3)\n",
        "\n",
        "    # Flatten encoding\n",
        "    encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "    num_pixels = encoder_out.size(1)\n",
        "\n",
        "    # We'll treat the problem as having a batch size of k\n",
        "    encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "    # Tensor to store top k previous words at each step; now they're just <start>\n",
        "    k_prev_words = torch.LongTensor([[vocab['<start>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences; now they're just <start>\n",
        "    seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' scores; now they're just 0\n",
        "    top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "    # Tensor to store top k sequences' alphas; now they're just 1s\n",
        "    seqs_alpha = torch.ones(k, 1, enc_image_size, enc_image_size).to(device)  # (k, 1, enc_image_size, enc_image_size)\n",
        "\n",
        "    # Lists to store completed sequences, their alphas and scores\n",
        "    complete_seqs = list()\n",
        "    complete_seqs_alpha = list()\n",
        "    complete_seqs_scores = list()\n",
        "\n",
        "    # Start decoding\n",
        "    step = 1\n",
        "    h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "    # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "    while True:\n",
        "\n",
        "        embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "        awe, alpha = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "        alpha = alpha.view(-1, enc_image_size, enc_image_size)  # (s, enc_image_size, enc_image_size)\n",
        "\n",
        "        gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "        awe = gate * awe\n",
        "\n",
        "        h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "        scores = decoder.fc(h)  # (s, vocab_size)\n",
        "        scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "        # Add\n",
        "        \n",
        "        scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "        \n",
        "        # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "        if step == 1:\n",
        "            top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "        else:\n",
        "            # Unroll and find top scores, and their unrolled indices\n",
        "            top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "        # Convert unrolled indices to actual indices of scores\n",
        "        prev_word_inds = (top_k_words / vocab_size).long()  # (s)\n",
        "        next_word_inds = top_k_words % vocab_size # (s)\n",
        "\n",
        "        # Add new words to sequences, alphas\n",
        "        seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "        seqs_alpha = torch.cat([seqs_alpha[prev_word_inds], alpha[prev_word_inds].unsqueeze(1)],\n",
        "                               dim=1)  # (s, step+1, enc_image_size, enc_image_size)\n",
        "\n",
        "        # Which sequences are incomplete (didn't reach <end>)?\n",
        "        incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                           next_word != vocab['<end>']]\n",
        "        complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "        # Set aside complete sequences\n",
        "        if len(complete_inds) > 0:\n",
        "            complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "            complete_seqs_alpha.extend(seqs_alpha[complete_inds].tolist())\n",
        "            complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "        k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "        # Proceed with incomplete sequences\n",
        "        if k == 0:\n",
        "            break\n",
        "        seqs = seqs[incomplete_inds]\n",
        "        seqs_alpha = seqs_alpha[incomplete_inds]\n",
        "        h = h[prev_word_inds[incomplete_inds]]\n",
        "        c = c[prev_word_inds[incomplete_inds]]\n",
        "        encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "        top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "        k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "        # Break if things have been going on too long\n",
        "        if step > 50:\n",
        "            break\n",
        "        step += 1\n",
        "\n",
        "    i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "    seq = complete_seqs[i]\n",
        "    alphas = complete_seqs_alpha[i]\n",
        "\n",
        "    return seq, alphas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rLNkqZ-TUBBg"
      },
      "source": [
        "**4.2 Visualize attention**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nzRqAer2pgRf"
      },
      "source": [
        "Instead of investigating the \"true impact\" of attention score on each input images, we'll use some trick to visualize attentions on the input image.\n",
        "\n",
        "\n",
        "*   Resize image to 14\\*N X 14\\*N scale\n",
        "*   Expand our 14 X 14 attention score matrix to 14\\*N X 14\\*N like below. Expanded attentions matrix are acting like a regional mask on the input image.\n",
        "\n",
        "![picture](https://docs.opencv.org/3.4/Pyramids_Tutorial_Pyramid_Theory.png)\n",
        "* Plot image and attention mask in a single image.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zh-On1XUBjv"
      },
      "source": [
        "def visualize_att(image_path, seq, alphas, rev_vocab, smooth=True):\n",
        "    \"\"\"\n",
        "    Visualizes caption with weights at every word.\n",
        "    Adapted from paper authors' repo: https://github.com/kelvinxu/arctic-captions/blob/master/alpha_visualization.ipynb\n",
        "    :param image_path: path to image that has been captioned\n",
        "    :param seq: caption\n",
        "    :param alphas: weights\n",
        "    :param rev_vocab: reverse word mapping, i.e. ix2word\n",
        "    :param smooth: smooth weights?\n",
        "    \"\"\"\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize([14 * 24, 14 * 24], Image.LANCZOS)\n",
        "\n",
        "    words = [rev_vocab[ind] for ind in seq]\n",
        "\n",
        "    for t in range(len(words)):\n",
        "        if t > 50:\n",
        "            break\n",
        "        plt.subplot(np.ceil(len(words) / 5.), 5, t + 1)\n",
        "\n",
        "        plt.text(0, 1, '%s' % (words[t]), color='black', backgroundcolor='white', fontsize=12)\n",
        "        plt.imshow(image)\n",
        "        current_alpha = alphas[t, :]\n",
        "        if smooth:\n",
        "            alpha = skimage.transform.pyramid_expand(current_alpha.numpy(), upscale=24, sigma=8)\n",
        "        else:\n",
        "            alpha = skimage.transform.resize(current_alpha.numpy(), [14 * 24, 14 * 24])\n",
        "        if t == 0:\n",
        "            plt.imshow(alpha, alpha=0)\n",
        "        else:\n",
        "            plt.imshow(alpha, alpha=0.8)\n",
        "        plt.set_cmap(cm.Greys_r)\n",
        "        plt.axis('off')\n",
        "    plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GuEIIc2nUS2R"
      },
      "source": [
        "**4.3 Let's check our model really working well!**\n",
        "\n",
        "If you want to check other examples, please change \n",
        "```\n",
        "# IMG_PATH = 'COCO-toy/<image_name>'\n",
        "```\n",
        "List of images for qualitative evaluation:\n",
        "* baseball.jpg\n",
        "* baseball2.jpg\n",
        "* bathroom.jpg\n",
        "* bear.jpg\n",
        "* birds.jpg\n",
        "* city.jpg\n",
        "* dog.jpg\n",
        "* giraffe.jpg\n",
        "* laptop.jpg\n",
        "* man.jpg\n",
        "* man2.jpg\n",
        "* manandwoman.jpg\n",
        "* meal.jpg\n",
        "* motorcycle.jpg\n",
        "* people.jpg\n",
        "* pizza.jpg\n",
        "* room.jpg\n",
        "* snowboard.jpg\n",
        "* wine.jpg"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mx45BorYUTBa"
      },
      "source": [
        "# Change this\n",
        "IMG_PATH = 'COCO-toy/dog.jpg'\n",
        "MODEL = 'BEST_checkpoint_coco_5_cap_per_img_5_min_word_freq.pth.tar'\n",
        "vocab = 'WORDMAP_coco_5_cap_per_img_5_min_word_freq.json'\n",
        "BEAM_SIZE=3\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(MODEL, map_location=str(device))\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "# Load word map (word2ix)\n",
        "with open(vocab, 'r') as j:\n",
        "    vocab = json.load(j)\n",
        "rev_vocab = {v: k for k, v in vocab.items()}  # ix2word\n",
        "\n",
        "# Encode, decode with attention and beam search\n",
        "seq, alphas = caption_image_beam_search(encoder, decoder, IMG_PATH, vocab, BEAM_SIZE)\n",
        "alphas = torch.FloatTensor(alphas)\n",
        "\n",
        "# Visualize caption and attention of best sequence\n",
        "visualize_att(IMG_PATH, seq, alphas, rev_vocab)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYA7OD8Vxqom"
      },
      "source": [
        "# 5. Supplementary materials"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "em0aEJOYx1WD"
      },
      "source": [
        "**5-1. Evaluation with beam search**\n",
        "\n",
        "We can also test our model with a beam search.\n",
        "\n",
        "This is just a combination of section 3-3 and 4-1."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kxgaj2_CquDZ"
      },
      "source": [
        "import torch.backends.cudnn as cudnn\n",
        "import torch.optim\n",
        "import torch.utils.data\n",
        "import torchvision.transforms as transforms\n",
        "#from datasets import *\n",
        "from utils import *\n",
        "from nltk.translate.bleu_score import corpus_bleu\n",
        "import torch.nn.functional as F\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Parameters\n",
        "data_folder = 'cache_features'  # folder with data files saved by create_input_files.py\n",
        "data_name = 'flickr8k_5_cap_per_img_5_min_word_freq'  # base name shared by data files\n",
        "checkpoint = 'BEST_checkpoint_flickr8k_5_cap_per_img_5_min_word_freq.pth.tar'  # model checkpoint\n",
        "vocab_file = 'cache_features/WORDMAP_flickr8k_5_cap_per_img_5_min_word_freq.json'  # word map, ensure it's the same the data was encoded with and the model was trained with\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # sets device for model and PyTorch tensors\n",
        "cudnn.benchmark = True  # set to true only if inputs to model are fixed size; otherwise lot of computational overhead\n",
        "\n",
        "# Load model\n",
        "checkpoint = torch.load(checkpoint)\n",
        "decoder = checkpoint['decoder']\n",
        "decoder = decoder.to(device)\n",
        "decoder.eval()\n",
        "encoder = checkpoint['encoder']\n",
        "encoder = encoder.to(device)\n",
        "encoder.eval()\n",
        "\n",
        "# Load word map (word2ix)\n",
        "with open(vocab_file, 'r') as j:\n",
        "    vocab = json.load(j)\n",
        "rev_vocab = {v: k for k, v in vocab.items()}\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Normalization transform\n",
        "normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                                 std=[0.229, 0.224, 0.225])\n",
        "\n",
        "\n",
        "def evaluate(beam_size):\n",
        "    \"\"\"\n",
        "    Evaluation\n",
        "    :param beam_size: beam size at which to generate captions for evaluation\n",
        "    :return: BLEU-4 score\n",
        "    \"\"\"\n",
        "    # DataLoader\n",
        "    loader = torch.utils.data.DataLoader(\n",
        "        CaptionDataset(data_folder, data_name, 'TEST', transform=transforms.Compose([normalize])),\n",
        "        batch_size=1, shuffle=True, num_workers=1, pin_memory=True)\n",
        "\n",
        "    # TODO: Batched Beam Search\n",
        "    # Therefore, do not use a batch_size greater than 1 - IMPORTANT!\n",
        "\n",
        "    # Lists to store references (true captions), and hypothesis (prediction) for each image\n",
        "    # If for n images, we have n hypotheses, and references a, b, c... for each image, we need -\n",
        "    # references = [[ref1a, ref1b, ref1c], [ref2a, ref2b], ...], hypotheses = [hyp1, hyp2, ...]\n",
        "    references = list()\n",
        "    hypotheses = list()\n",
        "\n",
        "    # For each image\n",
        "    for i, (image, caps, caplens, allcaps) in enumerate(\n",
        "            tqdm(loader, desc=\"EVALUATING AT BEAM SIZE \" + str(beam_size))):\n",
        "\n",
        "        k = beam_size\n",
        "\n",
        "        # Move to GPU device, if available\n",
        "        image = image.to(device)  # (1, 3, 256, 256)\n",
        "\n",
        "        # Encode\n",
        "        encoder_out = encoder(image)  # (1, enc_image_size, enc_image_size, encoder_dim)\n",
        "        enc_image_size = encoder_out.size(1)\n",
        "        encoder_dim = encoder_out.size(3)\n",
        "\n",
        "        # Flatten encoding\n",
        "        encoder_out = encoder_out.view(1, -1, encoder_dim)  # (1, num_pixels, encoder_dim)\n",
        "        num_pixels = encoder_out.size(1)\n",
        "\n",
        "        # We'll treat the problem as having a batch size of k\n",
        "        encoder_out = encoder_out.expand(k, num_pixels, encoder_dim)  # (k, num_pixels, encoder_dim)\n",
        "\n",
        "        # Tensor to store top k previous words at each step; now they're just <start>\n",
        "        k_prev_words = torch.LongTensor([[vocab['<start>']]] * k).to(device)  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences; now they're just <start>\n",
        "        seqs = k_prev_words  # (k, 1)\n",
        "\n",
        "        # Tensor to store top k sequences' scores; now they're just 0\n",
        "        top_k_scores = torch.zeros(k, 1).to(device)  # (k, 1)\n",
        "\n",
        "        # Lists to store completed sequences and scores\n",
        "        complete_seqs = list()\n",
        "        complete_seqs_scores = list()\n",
        "\n",
        "        # Start decoding\n",
        "        step = 1\n",
        "        h, c = decoder.init_hidden_state(encoder_out)\n",
        "\n",
        "        # s is a number less than or equal to k, because sequences are removed from this process once they hit <end>\n",
        "        while True:\n",
        "\n",
        "            embeddings = decoder.embedding(k_prev_words).squeeze(1)  # (s, embed_dim)\n",
        "\n",
        "            awe, _ = decoder.attention(encoder_out, h)  # (s, encoder_dim), (s, num_pixels)\n",
        "\n",
        "            gate = decoder.sigmoid(decoder.f_beta(h))  # gating scalar, (s, encoder_dim)\n",
        "            awe = gate * awe\n",
        "\n",
        "            h, c = decoder.decode_step(torch.cat([embeddings, awe], dim=1), (h, c))  # (s, decoder_dim)\n",
        "\n",
        "            scores = decoder.fc(h)  # (s, vocab_size)\n",
        "            scores = F.log_softmax(scores, dim=1)\n",
        "\n",
        "            # Add\n",
        "            scores = top_k_scores.expand_as(scores) + scores  # (s, vocab_size)\n",
        "\n",
        "            # For the first step, all k points will have the same scores (since same k previous words, h, c)\n",
        "            if step == 1:\n",
        "                top_k_scores, top_k_words = scores[0].topk(k, 0, True, True)  # (s)\n",
        "            else:\n",
        "                # Unroll and find top scores, and their unrolled indices\n",
        "                top_k_scores, top_k_words = scores.view(-1).topk(k, 0, True, True)  # (s)\n",
        "\n",
        "            # Convert unrolled indices to actual indices of scores\n",
        "            prev_word_inds = (top_k_words / vocab_size).long()  # (s)\n",
        "            next_word_inds = top_k_words % vocab_size  # (s)\n",
        "\n",
        "            # Add new words to sequences\n",
        "            seqs = torch.cat([seqs[prev_word_inds], next_word_inds.unsqueeze(1)], dim=1)  # (s, step+1)\n",
        "\n",
        "            # Which sequences are incomplete (didn't reach <end>)?\n",
        "            incomplete_inds = [ind for ind, next_word in enumerate(next_word_inds) if\n",
        "                               next_word != vocab['<end>']]\n",
        "            complete_inds = list(set(range(len(next_word_inds))) - set(incomplete_inds))\n",
        "\n",
        "            # Set aside complete sequences\n",
        "            if len(complete_inds) > 0:\n",
        "                complete_seqs.extend(seqs[complete_inds].tolist())\n",
        "                complete_seqs_scores.extend(top_k_scores[complete_inds])\n",
        "            k -= len(complete_inds)  # reduce beam length accordingly\n",
        "\n",
        "            # Proceed with incomplete sequences\n",
        "            if k == 0:\n",
        "                break\n",
        "            seqs = seqs[incomplete_inds]\n",
        "            h = h[prev_word_inds[incomplete_inds]]\n",
        "            c = c[prev_word_inds[incomplete_inds]]\n",
        "            encoder_out = encoder_out[prev_word_inds[incomplete_inds]]\n",
        "            top_k_scores = top_k_scores[incomplete_inds].unsqueeze(1)\n",
        "            k_prev_words = next_word_inds[incomplete_inds].unsqueeze(1)\n",
        "\n",
        "            # Break if things have been going on too long\n",
        "            if step > 50:\n",
        "                break\n",
        "            step += 1\n",
        "\n",
        "        i = complete_seqs_scores.index(max(complete_seqs_scores))\n",
        "        seq = complete_seqs[i]\n",
        "\n",
        "        # References\n",
        "        img_caps = allcaps[0].tolist()\n",
        "        img_captions = list(\n",
        "            map(lambda c: [w for w in c if w not in {vocab['<start>'], vocab['<end>'], vocab['<pad>']}],\n",
        "                img_caps))  # remove <start> and pads\n",
        "        references.append(img_captions)\n",
        "\n",
        "        # Hypotheses\n",
        "        hypotheses.append([w for w in seq if w not in {vocab['<start>'], vocab['<end>'], vocab['<pad>']}])\n",
        "\n",
        "        assert len(references) == len(hypotheses)\n",
        "\n",
        "    # Calculate BLEU-4 scores\n",
        "    bleu4 = corpus_bleu(references, hypotheses)\n",
        "\n",
        "    return bleu4\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    beam_size = 1\n",
        "    print(\"\\nBLEU-4 score @ beam size of %d is %.4f.\" % (beam_size, evaluate(beam_size)))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}