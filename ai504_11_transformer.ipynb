{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "colab": {
      "name": "ai504_week11_transformer_practice.ipynb",
      "provenance": [],
      "private_outputs": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "okJXS1-OEbN7"
      },
      "source": [
        "# Table of contents\n",
        "1. [Prepare input](#1)\n",
        "2. [Implement Transformer](#2)\n",
        "3. [Train and Evaluate](#3)\n",
        "4. [Visualize attention](#4)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F2Bj-rF9EbN7"
      },
      "source": [
        "# Prepare essential packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nHrI30vIEbN7"
      },
      "source": [
        "%matplotlib inline\n",
        "!pip install --upgrade torchtext\n",
        "!git clone https://github.com/sjpark9503/attentionviz.git\n",
        "!python -m spacy download de\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sayXpp8FEbN8"
      },
      "source": [
        "# I. Prepare input\n",
        "<a id='1'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KWOpLYfEbN8"
      },
      "source": [
        "We've already learned how to preprocess the text data in week 8, 9 & 10.\n",
        "\n",
        "You can see some detailed explanation about translation datasets in [torchtext](https://pytorch.org/text/), [practice session,week 9](https://classum.com/main/course/7726/103) and [PyTorch NMT tutorial](https://pytorch.org/tutorials/beginner/torchtext_translation_tutorial.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "kaduS25kEbN8"
      },
      "source": [
        "import torch\n",
        "from torchtext.datasets import Multi30k\n",
        "from torchtext.data import Field, BucketIterator\n",
        "\n",
        "SRC = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"de\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            batch_first=True,\n",
        "            lower = True)\n",
        "\n",
        "TRG = Field(tokenize = \"spacy\",\n",
        "            tokenizer_language=\"en\",\n",
        "            init_token = '<sos>',\n",
        "            eos_token = '<eos>',\n",
        "            batch_first=True,\n",
        "            lower = True)\n",
        "\n",
        "train_data, valid_data, test_data = Multi30k.splits(exts = ('.de', '.en'),\n",
        "                                                    fields = (SRC, TRG))\n",
        "\n",
        "SRC.build_vocab(train_data, min_freq = 2)\n",
        "TRG.build_vocab(train_data, min_freq = 2)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "BATCH_SIZE = 128\n",
        "\n",
        "train_iterator, valid_iterator, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device,\n",
        "    shuffle=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE_u1Qg-EbN8"
      },
      "source": [
        "# II. Implement Transformer\n",
        "<a id='2'></a>\n",
        "In practice week 11, we will learn how to implement the __[Attention is all you need](https://papers.nips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf) (Vaswani et al., 2017)__\n",
        "\n",
        "The overall architecutre is as follows:\n",
        "![picture](http://incredible.ai/assets/images/transformer-architecture.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqEVVfl-EbN8"
      },
      "source": [
        "## 1. Basic building blocks\n",
        "\n",
        "In this sections, we will implement the building blocks of the transformer: [Multi-head attention](#1a), [Position wise feedforward network](#1b) and [Positional encoding](#1c)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XeI2oINrEbN8"
      },
      "source": [
        "### a. Attention\n",
        "<a id='1a'></a>\n",
        "In this section, you will implement scaled dot-product attention and multi-head attention.\n",
        "\n",
        "__Scaled dot product:__\n",
        "\n",
        "![picture](http://incredible.ai/assets/images/transformer-scaled-dot-product.png)\n",
        "\n",
        "__Multi-head attention:__\n",
        "\n",
        "![picture](http://jalammar.github.io/images/t/transformer_multi-headed_self-attention-recap.png)\n",
        "Equation:\n",
        "\n",
        "$$\\begin{align} \\text{MultiHead}(Q, K, V) &= \\text{Concat}(head_1, ...., head_h) W^O \\\\\n",
        "\\text{where head}_i &= \\text{Attention} \\left( QW^Q_i, K W^K_i, VW^v_i \\right)\n",
        "\\end{align}$$\n",
        "\n",
        "__Query, Key and Value projection:__\n",
        "\n",
        "![picture](http://jalammar.github.io/images/t/self-attention-matrix-calculation.png)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "07AkqQcqEbN8"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import time\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\"Multi-headed attention from 'Attention Is All You Need' paper\"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        emb_dim,\n",
        "        num_heads,\n",
        "        dropout=0.0,\n",
        "        bias=False,\n",
        "        encoder_decoder_attention=False,  # otherwise self_attention\n",
        "        causal = False\n",
        "    ):\n",
        "        super().__init__()\n",
        "        self.emb_dim = emb_dim\n",
        "        self.num_heads = num_heads\n",
        "        self.dropout = dropout\n",
        "        self.head_dim = emb_dim // num_heads\n",
        "        assert self.head_dim * num_heads == self.emb_dim, \"emb_dim must be divisible by num_heads\"\n",
        "\n",
        "        self.encoder_decoder_attention = encoder_decoder_attention\n",
        "        self.causal = causal\n",
        "        self.k_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.v_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.q_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "        self.out_proj = nn.Linear(emb_dim, emb_dim, bias=bias)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        \"\"\"\n",
        "        To-Do : Reshape input\n",
        "          Args : batch_size X sequence_length X embedding dimension\n",
        "          Return : batch_size X # attention head X sequence_length X head dimension\n",
        "        \"\"\"\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "        # This is equivalent to\n",
        "        # return x.transpose(1,2)\n",
        "    \n",
        "    def scaled_dot_product(self, \n",
        "                           query: torch.Tensor, \n",
        "                           key: torch.Tensor, \n",
        "                           value: torch.Tensor,\n",
        "                           attention_mask: torch.BoolTensor):\n",
        "        \"\"\"\n",
        "        To-Do : Implement scaled dot product\n",
        "          Args:\n",
        "            Query (Tensor): shape `(batch, seq_len, emb_dim)`\n",
        "            Key (Tensor): shape `(batch, seq_len, emb_dim)`\n",
        "            Value (Tensor): shape `(batch, seq_len, emb_dim)`\n",
        "            attention_mask: binary BoolTensor of shape `(batch, seq_len)` or `(seq_len, seq_len)`\n",
        "\n",
        "          Returns:\n",
        "            attn_output : attended output (result of attention mechanism)\n",
        "            attn_weights: value of each attention\n",
        "        \"\"\"\n",
        "        return attn_output, attn_weights\n",
        "    \n",
        "    def MultiHead_scaled_dot_product(self, \n",
        "                       query: torch.Tensor, \n",
        "                       key: torch.Tensor, \n",
        "                       value: torch.Tensor,\n",
        "                       attention_mask: torch.BoolTensor):\n",
        "        \"\"\"\n",
        "        To-Do : Implement Multi-head version of scaled dot product, please also take the causal masking into account.\n",
        "          Args:\n",
        "            Query (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
        "            Key (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
        "            Value (Tensor): shape `(batch,# attention head, seq_len, head_dim)`\n",
        "            attention_mask: binary BoolTensor of shape `(batch, src_len)` or `(seq_len, seq_len)`\n",
        "\n",
        "          Returns:\n",
        "            attn_output : attended output (result of attention mechanism)\n",
        "            attn_weights: value of each attention\n",
        "        \"\"\"\n",
        "\n",
        "        return attn_output, attn_weights\n",
        "\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        query: torch.Tensor,\n",
        "        key: torch.Tensor,\n",
        "        attention_mask: torch.Tensor = None,\n",
        "        ):\n",
        "        q = self.q_proj(query)\n",
        "        # Enc-Dec attention\n",
        "        if self.encoder_decoder_attention:\n",
        "            k = self.k_proj(key)\n",
        "            v = self.v_proj(key)\n",
        "        # Self attention\n",
        "        else:\n",
        "            k = self.k_proj(query)\n",
        "            v = self.v_proj(query)\n",
        "\n",
        "        q = self.transpose_for_scores(q)\n",
        "        k = self.transpose_for_scores(k)\n",
        "        v = self.transpose_for_scores(v)\n",
        "\n",
        "        attn_output, attn_weights = self.MultiHead_scaled_dot_product(q,k,v,attention_mask)\n",
        "        return attn_output, attn_weights\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b528gtwHEbN8"
      },
      "source": [
        "### b. Position-wise feed forward network\n",
        "<a id='1b'></a>\n",
        "In this section, we will implement position-wise feed forward network\n",
        "\n",
        "$$\\text{FFN}(x) = \\max \\left(0, x W_1 + b_1 \\right) W_2 + b_2$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "sBqWWdIyEbN8"
      },
      "source": [
        "class PositionWiseFeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, emb_dim: int, d_ff: int, dropout: float = 0.1):\n",
        "        super(PositionWiseFeedForward, self).__init__()\n",
        "\n",
        "        self.activation = nn.ReLU()\n",
        "        self.w_1 = nn.Linear(emb_dim, d_ff)\n",
        "        self.w_2 = nn.Linear(d_ff, emb_dim)\n",
        "        self.dropout = dropout\n",
        "\n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        To-Do : Implement position-wise feed forward network\n",
        "          Args:\n",
        "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
        "        \"\"\"\n",
        "        return x + residual"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H9-qkoUKEbN8"
      },
      "source": [
        "### c. Sinusoidal Positional Encoding\n",
        "<a id='1c'></a>\n",
        "In this section, we will implement sinusoidal positional encoding\n",
        "\n",
        "$$\\begin{align}\n",
        "PE(pos, 2i) &= \\sin \\left( pos / 10000^{2i / d_{model}} \\right)  \\\\\n",
        "PE(pos, 2i+1) &= \\cos \\left( pos / 10000^{2i / d_{model}} \\right)  \n",
        "\\end{align}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tsiJalEvEbN8"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "class SinusoidalPositionalEmbedding(nn.Embedding):\n",
        "    def __init__(self, num_positions, embedding_dim, padding_idx=None):\n",
        "        super().__init__(num_positions, embedding_dim)\n",
        "        self.weight = self._init_weight(self.weight)\n",
        "    \n",
        "    @staticmethod\n",
        "    def _init_weight(out: nn.Parameter):\n",
        "        n_pos, embed_dim = out.shape\n",
        "        pe = nn.Parameter(torch.zeros(out.shape))\n",
        "        for pos in range(n_pos):\n",
        "            for i in range(0, embed_dim, 2):\n",
        "              \"\"\"\n",
        "              To-Do : Implement sinusoidal positional encoding\n",
        "              \"\"\"\n",
        "        pe.detach_()\n",
        "                \n",
        "        return pe\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, input_ids):\n",
        "        bsz, seq_len = input_ids.shape[:2]\n",
        "        positions = torch.arange(seq_len, dtype=torch.long, device=self.weight.device)\n",
        "        return super().forward(positions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RdhwI3hPEbN8"
      },
      "source": [
        "## 2. Transformer Encoder\n",
        "\n",
        "Now we have all basic building blocks which are essential to build Transformer. \n",
        "\n",
        "Let's implement Transformer step-by-step"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_6ym2hKzEbN8"
      },
      "source": [
        "### a. Encoder layer\n",
        "In this section, we will implement single layer of Transformer encoder.\n",
        "![picture](https://www.researchgate.net/publication/334288604/figure/fig1/AS:778232232148992@1562556431066/The-Transformer-encoder-structure.ppm)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "6B93kjUlEbN8"
      },
      "source": [
        "class EncoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config.emb_dim\n",
        "        self.ffn_dim = config.ffn_dim\n",
        "        self.self_attn = MultiHeadAttention(            \n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads, \n",
        "            dropout=config.attention_dropout)\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.dropout = config.dropout\n",
        "        self.activation_fn = nn.ReLU()\n",
        "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "    def forward(self, x, encoder_padding_mask):\n",
        "        \"\"\"\n",
        "        To-Do : Implement transformer encoder layer\n",
        "          Args:\n",
        "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
        "            encoder_padding_mask: binary BoolTensor of shape `(batch, src_len)`\n",
        "\n",
        "          Returns:\n",
        "            x : encoded output of shape `(batch, seq_len, emb_dim)`\n",
        "            self_attn_weights: self attention score\n",
        "        \"\"\"\n",
        "        return x, attn_weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LygNGzM0EbN8"
      },
      "source": [
        "### b. Encoder\n",
        "\n",
        "Stack encoder layers and build full Transformer encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "nZOAlAv7EbN8"
      },
      "source": [
        "class Encoder(nn.Module):\n",
        "    def __init__(self, config, embed_tokens):\n",
        "        super().__init__()\n",
        "\n",
        "        self.dropout = config.dropout\n",
        "\n",
        "        emb_dim = embed_tokens.embedding_dim\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_source_positions = config.max_position_embeddings\n",
        "\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "                config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
        "            )\n",
        "\n",
        "        self.layers = nn.ModuleList([EncoderLayer(config) for _ in range(config.encoder_layers)])\n",
        "\n",
        "    def forward(self, input_ids, attention_mask=None):\n",
        "        \"\"\"\n",
        "        To-Do : Implement the transformer encoder\n",
        "          Args:\n",
        "            input_ids (Tensor): input to the layer of shape `(batch, seq_len)`\n",
        "            attention_mask: binary BoolTensor of shape `(batch, src_len)`\n",
        "\n",
        "          Returns:\n",
        "            x: encoded output of shape `(batch, seq_len, emb_dim)`\n",
        "            self_attn_scores: a list of self attention score of each layer\n",
        "        \"\"\"\n",
        "\n",
        "        return x, self_attn_scores\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BgjqDJnKEbN8"
      },
      "source": [
        "## 3. Transformer Decoder"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "73LEB0mBEbN8"
      },
      "source": [
        "### a.Decoder layer\n",
        "In this section, we will implement single layer of Transformer decoder.\n",
        "![picture](http://incredible.ai/assets/images/transformer-decoder.png)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "-HgMu2QCEbN8"
      },
      "source": [
        "class DecoderLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super().__init__()\n",
        "        self.emb_dim = config.emb_dim\n",
        "        self.ffn_dim = config.ffn_dim\n",
        "        self.self_attn = MultiHeadAttention(\n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            causal=True,\n",
        "        )\n",
        "        self.dropout = config.dropout\n",
        "        self.self_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.encoder_attn = MultiHeadAttention(\n",
        "            emb_dim=self.emb_dim,\n",
        "            num_heads=config.attention_heads,\n",
        "            dropout=config.attention_dropout,\n",
        "            encoder_decoder_attention=True,\n",
        "        )\n",
        "        self.encoder_attn_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "        self.PositionWiseFeedForward = PositionWiseFeedForward(self.emb_dim, self.ffn_dim, config.dropout)\n",
        "        self.final_layer_norm = nn.LayerNorm(self.emb_dim)\n",
        "\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        x,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask=None,\n",
        "        causal_mask=None,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        To-Do : Implement the transformer decoder layer\n",
        "          Args:\n",
        "            x (Tensor): input to the layer of shape `(batch, seq_len, emb_dim)`\n",
        "            encoder_hidden_states: output from the encoder, used for\n",
        "                encoder-side attention\n",
        "            encoder_attention_mask: binary BoolTensor of shape `(batch, src_len)` to mask out encoder padding\n",
        "            causal_mask: binary BoolTensor of shape `(batch, src_len)` to mask out future tokens in decoder.\n",
        "\n",
        "\n",
        "          Returns:\n",
        "            x: decoded output of shape `(batch, seq_len, emb_dim)`\n",
        "            self_attn_weights: self attention score\n",
        "            cross_attn_weights: encoder-decoder attention score\n",
        "        \"\"\"\n",
        "        return (\n",
        "            x,\n",
        "            self_attn_weights,\n",
        "            cross_attn_weights,\n",
        "        ) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sAJQ-q5mEbN8"
      },
      "source": [
        "### b. Decoder\n",
        "\n",
        "Stack decoder layers and build full Transformer decoder.\n",
        "\n",
        "Unlike the encoder, you need to do one more job: pass the causal(unidirectional) mask to the decoder self attention layer "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "gEMa6owhEbN8"
      },
      "source": [
        "class Decoder(nn.Module):\n",
        "    \"\"\"\n",
        "    Transformer decoder consisting of *config.decoder_layers* layers. Each layer is a :class:`DecoderLayer`\n",
        "\n",
        "    Args:\n",
        "        config: BartConfig\n",
        "        embed_tokens (torch.nn.Embedding): output embedding\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, config, embed_tokens: nn.Embedding):\n",
        "        super().__init__()\n",
        "        self.dropout = config.dropout\n",
        "        self.padding_idx = embed_tokens.padding_idx\n",
        "        self.max_target_positions = config.max_position_embeddings\n",
        "        self.embed_tokens = embed_tokens\n",
        "        self.embed_positions = SinusoidalPositionalEmbedding(\n",
        "            config.max_position_embeddings, config.emb_dim, self.padding_idx\n",
        "        )\n",
        "        self.layers = nn.ModuleList([DecoderLayer(config) for _ in range(config.decoder_layers)])  # type: List[DecoderLayer]\n",
        "\n",
        "    def forward(\n",
        "        self,\n",
        "        input_ids,\n",
        "        encoder_hidden_states,\n",
        "        encoder_attention_mask,\n",
        "        decoder_causal_mask,\n",
        "    ):\n",
        "        \"\"\"\n",
        "        To-Do : Implement the transformer decoder\n",
        "\n",
        "        Args:\n",
        "            input_ids (LongTensor): previous decoder outputs of shape\n",
        "                `(batch, tgt_len)`, for teacher forcing\n",
        "            encoder_hidden_states: output from the encoder, used for\n",
        "                encoder-side attention\n",
        "            encoder_attention_mask: binary BoolTensor of shape `(batch, src_len)` to mask out encoder padding\n",
        "            causal_mask: binary BoolTensor of shape `(batch, src_len)` to mask out future tokens in decoder.\n",
        "\n",
        "          Returns:\n",
        "            x: decoded output of shape `(batch, seq_len, emb_dim)`\n",
        "            cross_attn_scores: list of encoder-decoder attention score of each layer\n",
        "        \"\"\"\n",
        "\n",
        "        return x, cross_attention_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lr0g3oeIEbN8"
      },
      "source": [
        "## 4. Transformer\n",
        "\n",
        "Let's combine encoder and decoder in one place!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "M4aZzq8GEbN8"
      },
      "source": [
        "class Transformer(nn.Module):\n",
        "    def __init__(self, SRC,TRG,config):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.SRC = SRC\n",
        "        self.TRG = TRG\n",
        "        \n",
        "        self.enc_embedding = nn.Embedding(len(SRC.vocab), config.emb_dim, padding_idx=SRC.vocab.stoi['<pad>'])\n",
        "        self.dec_embedding = nn.Embedding(len(TRG.vocab), config.emb_dim, padding_idx=TRG.vocab.stoi['<pad>'])\n",
        "\n",
        "        self.encoder = Encoder(config, self.enc_embedding)\n",
        "        self.decoder = Decoder(config, self.dec_embedding)\n",
        "        \n",
        "        self.prediction_head = nn.Linear(config.emb_dim,len(TRG.vocab))\n",
        "        \n",
        "        self.init_weights()\n",
        "        \n",
        "    def generate_mask(self,src,trg):\n",
        "        \"\"\"\n",
        "        To-Do : Generate mask for encoder and decoder attention.\n",
        "\n",
        "        Args:\n",
        "            src(LongTensor): Input to the transformer of shape (batch_size, seq_len)  \n",
        "            trg(LongTensor): Decoding target of the transformer of shape (batch_size, seq_len)  \n",
        "\n",
        "          Returns:\n",
        "            enc_attention_mask: padding mask for encoder\n",
        "            dec_attention_mask: causal mask for decoder\n",
        "        \"\"\"\n",
        "        return enc_attention_mask, dec_attention_mask\n",
        "        \n",
        "    def init_weights(self):\n",
        "        for name, param in self.named_parameters():\n",
        "            if param.requires_grad:\n",
        "                if 'weight' in name:\n",
        "                    nn.init.normal_(param.data, mean=0, std=0.01)\n",
        "                else:\n",
        "                    nn.init.constant_(param.data, 0)\n",
        "        \n",
        "    def forward(\n",
        "        self,\n",
        "        src,\n",
        "        trg,\n",
        "    ):\n",
        "        enc_attention_mask, dec_causal_mask = self.generate_mask(src, trg)\n",
        "        encoder_output, encoder_attention_scores = self.encoder(\n",
        "                input_ids=src,\n",
        "                attention_mask=enc_attention_mask\n",
        "            )\n",
        "\n",
        "        # decoder outputs consists of (dec_features, layer_state, dec_hidden, dec_attn)\n",
        "        decoder_output, decoder_attention_scores = self.decoder(\n",
        "            trg,\n",
        "            encoder_output,\n",
        "            encoder_attention_mask=enc_attention_mask,\n",
        "            decoder_causal_mask=dec_causal_mask,\n",
        "        )\n",
        "        decoder_output = self.prediction_head(decoder_output)\n",
        "\n",
        "        return decoder_output, encoder_attention_scores, decoder_attention_scores"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WU-llE39EbN8"
      },
      "source": [
        "# III. Train & Evaluate\n",
        "<a id='3'></a>\n",
        "This section is very similar to week 9, so please refer to it for detailed description."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ZRMlUmxEbN8"
      },
      "source": [
        "## 1. Configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "BlIc_VKaEbN8"
      },
      "source": [
        "import easydict\n",
        "\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "config = easydict.EasyDict({\n",
        "    \"emb_dim\":64,\n",
        "    \"ffn_dim\":256,\n",
        "    \"attention_heads\":4,\n",
        "    \"attention_dropout\":0.0,\n",
        "    \"dropout\":0.2,\n",
        "    \"max_position_embeddings\":512,\n",
        "    \"encoder_layers\":3,\n",
        "    \"decoder_layers\":3,\n",
        "    \n",
        "})\n",
        "\n",
        "N_EPOCHS = 100\n",
        "learning_rate = 5e-4\n",
        "CLIP = 1\n",
        "PAD_IDX = TRG.vocab.stoi['<pad>']\n",
        "\n",
        "model = Transformer(SRC,TRG,config)\n",
        "model.to(device)\n",
        "optimizer = optim.Adam(model.parameters(),lr=learning_rate)\n",
        "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
        "            \n",
        "best_valid_loss = float('inf')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1hql5wOKEbN8"
      },
      "source": [
        "## 2. Train & Eval"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "F1HHCxXuEbN8"
      },
      "source": [
        "import math\n",
        "import time\n",
        "from tqdm import tqdm\n",
        "\n",
        "\n",
        "def train(model: nn.Module,\n",
        "          iterator: BucketIterator,\n",
        "          optimizer: optim.Optimizer,\n",
        "          criterion: nn.Module,\n",
        "          clip: float):\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    for idx, batch in enumerate(iterator):\n",
        "\n",
        "        src = batch.src\n",
        "        trg = batch.trg\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        output, enc_attention_scores, _ = model(src, trg)\n",
        "\n",
        "        output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
        "        trg = trg[:,1:].reshape(-1)\n",
        "\n",
        "        loss = criterion(output, trg)\n",
        "\n",
        "        loss.backward()\n",
        "\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
        "\n",
        "        optimizer.step()\n",
        "\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "\n",
        "def evaluate(model: nn.Module,\n",
        "             iterator: BucketIterator,\n",
        "             criterion: nn.Module):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    epoch_loss = 0\n",
        "\n",
        "    with torch.no_grad():\n",
        "\n",
        "        for _, batch in enumerate(iterator):\n",
        "\n",
        "            src = batch.src\n",
        "            trg = batch.trg\n",
        "\n",
        "            output, attention_score, _ = model(src, trg) #turn off teacher forcing\n",
        "\n",
        "            output = output[:,:-1,:].reshape(-1, output.shape[-1])\n",
        "            trg = trg[:,1:].reshape(-1)\n",
        "\n",
        "            loss = criterion(output, trg)\n",
        "\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "    return epoch_loss / len(iterator)\n",
        "\n",
        "for epoch in tqdm(range(N_EPOCHS), total=N_EPOCHS):\n",
        "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
        "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
        "    \n",
        "    if best_valid_loss < valid_loss:\n",
        "        break\n",
        "    else:\n",
        "        best_valid_loss = valid_loss\n",
        "\n",
        "    print(f'\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n",
        "    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n",
        "\n",
        "test_loss = evaluate(model, test_iterator, criterion)\n",
        "\n",
        "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxyJad1WEbN8"
      },
      "source": [
        "# IV. Visualization\n",
        "<a id='4'></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_Eop7pGEbN8"
      },
      "source": [
        "## 1. Positional embedding visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "QJKGr5JfEbN8"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "# Visualization\n",
        "fig, ax = plt.subplots(figsize=(15, 9))\n",
        "cax = ax.matshow(model.encoder.embed_positions.weight.data.cpu().numpy(), aspect='auto',cmap=plt.cm.YlOrRd)\n",
        "fig.colorbar(cax)\n",
        "ax.set_title('Positional Embedding Matrix', fontsize=18)\n",
        "ax.set_xlabel('Embedding Dimension', fontsize=14)\n",
        "ax.set_ylabel('Sequence Length', fontsize=14)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BrLCVOWlEbN8"
      },
      "source": [
        "## 2. Attention visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "azwmQfF-EbN8"
      },
      "source": [
        "from attentionviz import head_view\n",
        "\n",
        "BATCH_SIZE = 1\n",
        "train_iterator, _, test_iterator = BucketIterator.splits(\n",
        "    (train_data, valid_data, test_data),\n",
        "    batch_size = BATCH_SIZE,\n",
        "    device = device)\n",
        "\n",
        "model.eval()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "tLCz7R73EbN8"
      },
      "source": [
        "import sys\n",
        "if not 'attentionviz' in sys.path:\n",
        "  sys.path += ['attentionviz']\n",
        "!pip install regex\n",
        "\n",
        "def call_html():\n",
        "  import IPython\n",
        "  display(IPython.core.display.HTML('''\n",
        "        <script src=\"/static/components/requirejs/require.js\"></script>\n",
        "        <script>\n",
        "          requirejs.config({\n",
        "            paths: {\n",
        "              base: '/static/base',\n",
        "              \"d3\": \"https://cdnjs.cloudflare.com/ajax/libs/d3/3.5.8/d3.min\",\n",
        "              jquery: '//ajax.googleapis.com/ajax/libs/jquery/2.0.0/jquery.min',\n",
        "            },\n",
        "          });\n",
        "        </script>\n",
        "        '''))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "RkV8XEM2EbN9"
      },
      "source": [
        "SAMPLE_IDX = 131\n",
        "\n",
        "with torch.no_grad():\n",
        "  for idx,example in enumerate(test_iterator):\n",
        "    if idx == SAMPLE_IDX:\n",
        "      sample = example\n",
        "  src = sample.src\n",
        "  trg = sample.trg\n",
        "\n",
        "  output, enc_attention_score, dec_attention_score = model(src, trg) #turn off teacher forcing\n",
        "  attention_score = {'self':enc_attention_score, 'cross':dec_attention_score}\n",
        "\n",
        "  src_tok = [SRC.vocab.itos[x] for x in src.squeeze()]\n",
        "  trg_tok = [TRG.vocab.itos[x] for x in trg.squeeze()]\n",
        "\n",
        "  call_html()\n",
        "  head_view(attention_score, src_tok, trg_tok)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}